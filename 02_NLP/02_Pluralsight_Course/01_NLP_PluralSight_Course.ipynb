{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tasks in Natural language Processing\n",
    "2. Tokenization\n",
    "3. Stop Words Removal\n",
    "4. Identifying N-Grams\n",
    "5. Stemming\n",
    "6. POS tagging\n",
    "7. Word Sense Disambiguation\n",
    "------------------------------------------\n",
    "8. Auto-Summarizing Text\n",
    "    a) Downloading the page using BeautifulSoup\n",
    "    b) Preprocessing the text\n",
    "    c) Extracting the summary\n",
    "------------------------------------------\n",
    "9. Classifying Text using ML\n",
    "    a) K-Means clustering to find clusters\n",
    "    b) Using K-nearest to classify them into those clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tasks in Natural Language Processing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "• Tokenization: Breaking down textx into words and sentences.\n",
    "• Stop Word Removal: Filtering common words. ex: is,an,the,a,etc,.\n",
    "• N-Grams: Identifying commonly occuring group of words. \n",
    "    Ex: Treating \"New\" and \"York\" as one word - New York (BiGram)\n",
    "• Word Disambiguation: Indentifying same words used in different context. \n",
    "    Ex: These are really cool effects.\n",
    "        Give me a glass of cold water.\n",
    "• Parts of Speech: Identifying parts of speech.\n",
    "• Stemming: Removing ends of the words. Ex: Closed,Closes,Closing -> Close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    " #Run this for first time:    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is good.', 'We can imporve and do better.']\n",
      "['This', 'is', 'good', '.', 'We', 'can', 'imporve', 'and', 'do', 'better', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is good. We can imporve and do better.\"\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for the first time: \n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customeStopWords = set(stopwords.words('english')+list(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'good', 'We', 'imporve', 'better']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_WO_StopWords = [word for word in word_tokenize(text) if word not in customeStopWords]\n",
    "Words_WO_StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Identifying N-Grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(Words_WO_StopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('This', 'good'), 1),\n",
       " (('We', 'imporve'), 1),\n",
       " (('good', 'We'), 1),\n",
       " (('imporve', 'better'), 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(finder.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', 'clos', 'on', 'clos', 'night', 'when', 'she', 'was', 'in', 'the', 'mood', 'to', 'clos', '.']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Mary closed on closing night when she was in the mood to close.\"\n",
    "st = LancasterStemmer()\n",
    "stemmedWords = [st.stem(word) for word in word_tokenize(text2)]\n",
    "print(stemmedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this for first time\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('closed', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('closing', 'NN'),\n",
       " ('night', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('she', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mood', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('close', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run for the first time:\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.01') the lowest part of the musical range\n",
      "Synset('bass.n.02') the lowest part in polyphonic music\n",
      "Synset('bass.n.03') an adult male singer with the lowest voice\n",
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n",
      "Synset('freshwater_bass.n.01') any of various North American freshwater fish with lean flesh (especially of the genus Micropterus)\n",
      "Synset('bass.n.06') the lowest adult male singing voice\n",
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
      "Synset('bass.n.08') nontechnical name for any of numerous edible marine and freshwater spiny-finned fishes\n",
      "Synset('bass.s.01') having or denoting a low vocal or instrumental range\n"
     ]
    }
   ],
   "source": [
    "for ss in wn.synsets('bass'):\n",
    "    print(ss,ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sensel = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"),'bass')\n",
    "print(sensel,sensel.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
     ]
    }
   ],
   "source": [
    "sensel2 = lesk(word_tokenize(\"This sea bass was really hard to catch\"),'bass')\n",
    "print(sensel2,sensel2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Auto Summarizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Downloading the text:==================================\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticURL = \"https://www.washingtonpost.com/news/the-switch/wp/2016/10/18/the-pentagons-massive-new-telescope-is-designed-to-track-space-junk-and-watch-out-for-killer-asteroids/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextfromurl(url):\n",
    "    page = urllib.request.urlopen(url).read().decode('utf8','ignore')\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    text = ' '.join(map(lambda p: p.text,soup.find_all('article')))\n",
    "    return str(text.encode('ascii',errors='replace')).replace(\"?\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = getTextfromurl(staticURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On Tuesday, the Defense Departmenttook another significant step toward monitoring all of the cosmic junk swirling around in space, by deliveringa gigantic new telescope capable of seeing small objects from very far away.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b) Pre-processing the text:=================================\n",
    "sents = sent_tokenize(text.replace('.','. '))\n",
    "sents[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sent = word_tokenize(text.lower())\n",
    "len(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sent = [word for word in word_sent if word not in customeStopWords]\n",
    "len(word_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'space': 14, 'telescope': 8, 'debris': 7, 'satellites': 6, 'orbit': 6, 'objects': 6, 'air': 6, 'force': 6, 'around': 4, 'small': 4, ...})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c) Extracting Summary:=====================================\n",
    "from nltk.probability import FreqDist\n",
    "freq = FreqDist(word_sent)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['space', 'telescope', 'debris', 'satellites', 'orbit', 'objects', 'air', 'force', 'around', 'small']\n"
     ]
    }
   ],
   "source": [
    "# Getting n highest occuring words.\n",
    "from heapq import nlargest\n",
    "print(nlargest(10,freq,key=freq.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 52, 1: 8, 2: 33, 3: 16, 4: 1, 5: 2, 6: 44, 7: 65, 8: 56, 9: 51, 10: 24, 11: 12, 12: 12, 13: 16, 14: 23, 15: 43, 16: 24, 17: 41, 18: 67, 19: 31, 20: 59, 21: 42, 22: 38, 23: 27, 24: 25, 25: 35, 26: 21, 27: 20, 28: 79, 29: 27, 30: 12, 31: 7, 32: 3, 33: 8})\n"
     ]
    }
   ],
   "source": [
    "# creating sentence ranking dictionary:\n",
    "from collections import defaultdict\n",
    "ranking = defaultdict(int)\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    for w in word_tokenize(sent.lower()):\n",
    "        if w in freq:\n",
    "            ranking[i] += freq[w]\n",
    "print(ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 18, 7, 20]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting top 4 sentences on these scores.\n",
    "sents_idx = nlargest(4,ranking,key=ranking.get)\n",
    "sents_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Tuesday, the Defense Departmenttook another significant step toward monitoring all of the cosmic junk swirling around in space, by deliveringa gigantic new telescope capable of seeing small objects from very far away.',\n",
       " 'The telescope is a big improvement over the legacy ground-based optical telescopes that are used by the U. S.  Air Force, because it can search large areas of sky and also track very faint (small) objects in and around GEO, Brian Weeden, a Technical Advisor at the Secure World Foundation, wrote in an email.',\n",
       " 'The telescope wouldjoin another new space debris tracking technology known as the Space Fence, which is now being built by Bethesda-based Lockheed Martin.',\n",
       " 'Every military operation that takes place in the world today is critically dependent on space in one way or another, Air Force Gen.  John Hyten said in an interview earlier this year when he was the commander of the Air Force Space Command.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sents[i] for i in sorted(sents_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Final Function to summarize.\n",
    "def summarize(text, n):\n",
    "    text = text.replace('.','. ')\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    assert n <= len(sents)\n",
    "    word_sent = word_tokenize(text.lower())\n",
    "    _stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    \n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    \n",
    "    ranking = defaultdict(int)\n",
    "    \n",
    "    for i, sent in enumerate(sents):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i] += freq[w]\n",
    "    sents_idx = nlargest(n,ranking,key=ranking.get)\n",
    "    return [sents[j] for j in sorted(sents_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Tuesday, the Defense Departmenttook another significant step toward monitoring all of the cosmic junk swirling around in space, by deliveringa gigantic new telescope capable of seeing small objects from very far away.',\n",
       " 'The telescope is a big improvement over the legacy ground-based optical telescopes that are used by the U. S.  Air Force, because it can search large areas of sky and also track very faint (small) objects in and around GEO, Brian Weeden, a Technical Advisor at the Secure World Foundation, wrote in an email.',\n",
       " 'Every military operation that takes place in the world today is critically dependent on space in one way or another, Air Force Gen.  John Hyten said in an interview earlier this year when he was the commander of the Air Force Space Command.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Classifying Text using ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll scrap inshorts news articles and cluster them in different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting first 25 posts:\n",
    "def addArticles(url):\n",
    "    req = urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    client = urllib.request.urlopen(req)\n",
    "    webpage = client.read()\n",
    "    client.close()\n",
    "\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    articles = soup.findAll(\"div\",{\"itemprop\":\"articleBody\"})\n",
    "    for article in articles:\n",
    "        posts.append(article.text)\n",
    "\n",
    "    sc = list(soup.findAll('script'))\n",
    "    code = str(sc[-1].text[25:35])\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scraping further pages, require code viz. o/p of prev page.\n",
    "def addArticle2(url,code):\n",
    "    data = {'category':'','news_offset':code}\n",
    "    response = requests.post(url, data=data)\n",
    "    htmltext = list((response.json()).values())[1]\n",
    "    soup = BeautifulSoup(htmltext, 'html.parser')\n",
    "    articles = soup.findAll(\"div\",{\"itemprop\":\"articleBody\"})\n",
    "    for article in articles:\n",
    "        posts.append(article.text)\n",
    "    return list((response.json()).values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#making first call for first page.\n",
    "posts = []\n",
    "url = \"https://inshorts.com/en/read\"\n",
    "code = addArticles(url)\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "# Hit this code block n times for further n pages:\n",
    "url = \"https://inshorts.com/en/ajax/more_news\"\n",
    "code = addArticle2(url,code)\n",
    "print(len(posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have data, there are two methods for feature extraction:\n",
    "a. Term Frequency\n",
    "b. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5,min_df=2,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<209x991 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4018 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(posts)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering:\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 3,init = 'k-means++',max_iter=100,n_init=1,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 387.162\n",
      "Iteration  1, inertia 199.939\n",
      "Iteration  2, inertia 199.541\n",
      "Iteration  3, inertia 199.207\n",
      "Iteration  4, inertia 198.967\n",
      "Iteration  5, inertia 198.697\n",
      "Iteration  6, inertia 198.356\n",
      "Iteration  7, inertia 198.327\n",
      "Converged at iteration 7: center shift 0.000000e+00 within tolerance 9.811060e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=3, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([60, 76, 73], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(km.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text={}\n",
    "for i,cluster in enumerate(km.labels_):\n",
    "    oneDocument = posts[i]\n",
    "    if cluster not in text.keys():\n",
    "        text[cluster] = oneDocument\n",
    "    else:\n",
    "        text[cluster] += oneDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing analysis on these 3 clusters:\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = set(stopwords.words('english') + list(punctuation) +\n",
    "                 [\"million\",\"billion\",\"year\",\"millions\",\"billions\",\n",
    "                  \"y/y\",\"'s'\",\"''\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "counts={}\n",
    "for cluster in range(3):\n",
    "    word_sent = word_tokenize(text[cluster].lower())\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    freq = FreqDist(word_sent)\n",
    "    keywords[cluster] = nlargest(100,freq,key=freq.get)\n",
    "    counts[cluster]=freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_keys={}\n",
    "for cluster in range(3):\n",
    "    other_clusters=list(set(range(3)) - set([cluster]))\n",
    "    keys_other_cluster=set(keywords[other_clusters[0]]).union(set(keywords[other_clusters[1]]))\n",
    "    unique = set(keywords[cluster]) - keys_other_cluster\n",
    "    unique_keys[cluster] =nlargest(10,unique,key=counts[cluster].get)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['indian', 'modi', 'wrote', 'trump', 'kohli', 'captain', 'cricket', 'played', 'england', 'twitter'], 1: ['covid-19', 'cases', 'positive', 'hospital', 'patients', 'reported', 'lockdown', 'lakh', 'total', 'cm'], 2: ['arrested', 'used', 'dubey', 'vikas', 'singh', 'killed', 'sushant', 'house', 'allegedly', 'accused']}\n"
     ]
    }
   ],
   "source": [
    "print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make a model which will predict the class of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = posts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "#Training :\n",
    "classifier.fit(x,km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=vectorizer.transform([article.encode('ascii',errors='ignore')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x991 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

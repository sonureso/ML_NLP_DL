{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Topic:\n",
    "        01 - Theory\n",
    "        02 - Tokenization Examples\n",
    "        03 - Stemming\n",
    "        04 - Lemmatization (it accepts POS-tags as arguments.)\n",
    "        05 - POS-Tag\n",
    "        06 - nltk-Synonyms\n",
    "        07 - N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  01-Theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms:\n",
    "    1. Corpus, Tokens and N-Grams\n",
    "    2. Tokenization\n",
    "    3. Stemming\n",
    "    4. Lemmatization\n",
    "    5. Part of Speech Tagging\n",
    "    6. Dependency Grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Corpus: Collection of text documents.\n",
    "    Corpus > Documents > Paragraphs > Sentences > Tokens\n",
    "    \n",
    "♦ Tokens: Smaller units of text(words, phrases or ngrams).\n",
    "\n",
    "♦ N-grams: Combination of n words/characters together.\n",
    "    Ex: I love my phone\n",
    "    Unigrams(n=1): I, love, my, phone\n",
    "    Bigrams(n=2): I love, love my, my phone\n",
    "    Trigrams(n=3): I love my, love my phone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Tokenization: process of spliting a text object into smaller units(tokens)\n",
    "    Ex_1: White Space Tokenizer/Unigram Tokenizer\n",
    "        Sentence: \"I went to New-York to play football\"\n",
    "        Tokens: \"I\", \"went\", \"to\" ,\"New-York\", \"to\" ,\"play\", \"football\"\n",
    "    Ex_2: Regular Expression Tokenizer\n",
    "        Sentence: \"Football,Cricket;Baseball Tennis\"\n",
    "        re.split(r'[;,\\s]',st)\n",
    "        Tokens: ['Football', 'Cricket', 'Baseball', 'Tennis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Normalization: it is the process of converting a token into its base form(morpheme).\n",
    "    Morpheme: Base form of a word\n",
    "        Structure of Token: <prefix> <morpheme> <suffix> \n",
    "        Ex: Antinationalist = Anti + national + ist\n",
    "• Two methods of normalization are: stemming and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Stemming: Elementary rule based process of removal of inflectional forms from a token.\n",
    "    Ex: \"langhing\", \"Laughs\", \"Laughed\" > \"Laugh\"\n",
    "    But sometimes this method is not very efficient.\n",
    "    Ex: \"his teams are not winning\"  > \"hi\" \"team\" \"are\" \"not\" \"winn\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Lemmatization: Systematic process for reducing a token to its lemma. It uses grammers and parts of speech.\n",
    "    Ex: 1. is,am,are  > be\n",
    "        2. running,runs,ran > run\n",
    "        3. running(verb) > run\n",
    "        4. running(noun) > running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Parts of Speech Tagging(POS Tagging): Noun, Verb, Adjective, and adverbs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "♦ Grammer:\n",
    "    1. Constituency Grammer: Organize any sentence into constituents using their properties.\n",
    "        Sentence: <subject> <context> <object>\n",
    "            Ex: The dogs are barking in the park.\n",
    "                <subject>: The cat/the dogs/they\n",
    "                <context>: are barking/are running/are eating\n",
    "                <object>: in the park/ happily/since the morning.\n",
    "    2. Dependency Grammer: Words of a sentence depends on which other words(dependecies).\n",
    "            Modifier: Barking dog.(dog is modified by barking)\n",
    "            Relation(Governer,Relation,Dependent)\n",
    "            Ex: <AnalyticsVidhya><is><the largest community of data scientists>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-Tokenization Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Football', 'Cricket', 'Baseball', 'Tennis']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization Example:\n",
    "import re\n",
    "st = \"Football,Cricket;Baseball Tennis\"\n",
    "re.split(r'[;,\\s]',st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'John',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " '?',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'traveling',\n",
       " 'to',\n",
       " 'your',\n",
       " 'city',\n",
       " '.',\n",
       " 'Lets',\n",
       " 'Catchup',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk import sent_tokenize, word_tokenize #if doesn't work use: from nltk.tokenize import....\n",
    "#from mosestokenizer import sent_tokenize\n",
    "text = \"Hi John! How are you doing? I will be traveling to your city. Lets Catchup.\"\n",
    "sent_tokenize(text)\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "increas\n",
      "rain\n",
      "decreas\n"
     ]
    }
   ],
   "source": [
    "#Stemming is not very efficient: Sometimes it produces meaningless words:\n",
    "print(stemmer.stem(\"Playing\"))\n",
    "print(stemmer.stem(\"Increases\"))\n",
    "print(stemmer.stem(\"Raining\"))\n",
    "print(stemmer.stem(\"Decreases\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04- Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if doesn't work: First download wordnet: run below line\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing\n",
      "play\n",
      "increase\n"
     ]
    }
   ],
   "source": [
    "# this accept 2nd argument as POS tag.\n",
    "print(lemm.lemmatize(\"playing\"))\n",
    "print(lemm.lemmatize(\"playing\",pos=\"v\"))\n",
    "print(lemm.lemmatize(\"increases\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05- POS-Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "text = \"Hi John! How are you doing? I will be traveling to your city. Lets Catchup.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " ('!', '.'),\n",
       " ('How', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('traveling', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('your', 'PRP$'),\n",
       " ('city', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Lets', 'VBZ'),\n",
       " ('Catchup', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If Error, run below code for downloading: averaged_perceptron_tagger\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "tokens = word_tokenize(text)\n",
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-nltk - Synonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk we can get synonyms of words:\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets(\"Computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07- N-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love')\n",
      "('love', 'to')\n",
      "('to', 'play')\n",
      "('play', 'football')\n"
     ]
    }
   ],
   "source": [
    "# From nltk we can use ngrams:\n",
    "from nltk import ngrams\n",
    "sentence = \"I love to play football\"\n",
    "for gram in ngrams(word_tokenize(sentence),2):\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

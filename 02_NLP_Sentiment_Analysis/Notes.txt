Visit: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python
=====================================================================================
Stemming: Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the
          same stem even if the stem itself is not a valid word in the Language.
          
=====================================================================================
Lemmatization Vs Stemming: 
  Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, 
  lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last
  few characters, often leading to incorrect meanings and spelling errors.
  Ex: 
    ‘Caring’ -> Lemmatization -> ‘Care’
    ‘Caring’ -> Stemming -> ‘Car’
=====================================================================================
Tokenization: 
          Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as 
          individual words or terms. Each of these smaller units are called tokens
Ex:    “This is a cat.”    to     [‘This’, ‘is’, ‘a’, cat’].
            
=====================================================================================
Steps of NLP:
1. Sentence segmentation — split into sentences
2. Word tokenization — split into words
3. Text stemming and lemmatization — return the base form out of many forms a word can have(‘is’ becomes ‘be’)
4. Ignore stop words — ignore frequent words that do not add too much meaning(e.g. ‘the’, ‘a’)
5. Dependency parsing — identify the relationship between the words the root word and other words
6. Find noun phrases — remove extra detail(adjectives — attributes) and keep the main words
7. Named Entity Recognition — tag the words to some abstract object
8. Coreference Resolution — as we have a representation per sentence we have to relate the sentences to each other and convey the idea

======================================================================================



          
          
